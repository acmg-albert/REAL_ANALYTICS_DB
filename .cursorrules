# Instructions

During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Project Requirements

## Code Organization
- Each data source and database module should be completely independent
- Separate modules for each data table to avoid code coupling
- No module should affect the functionality of other modules
- Maintain clean separation of concerns

## Code Style
- All code, comments, database fields, and documentation must be in English
- No mixing of Chinese and English in code or documentation
- Keep naming conventions consistent throughout the project

## Execution Requirements
- All data scraping and import processes must show clear progress status
- Include logging for all operations
- Display clear success/failure messages
- Show progress bars for long-running operations

## Database Design
- Each table should have clear, English-named fields
- Use consistent naming conventions across all tables
- Include proper documentation for all database schemas

## Logging and Monitoring
- Log all operations with appropriate detail level
- Include timestamp and operation status
- Show clear progress indicators for all operations
- Maintain separate logs for different operations

# Lessons

## 项目路径 - 重要！
- 项目根目录是 `E:\Cursor_AI\REAL_ANALYTICS_DB`（使用反斜杠！）
- 不要使用 `E:/Cursor_AI/REAL_ANALYTICS_DB`（错误的正斜杠）
- 所有相对路径都应该相对于这个目录
- 在 PowerShell 中必须使用反斜杠 `\`
- 如果找不到文件，首先检查是否跳出了项目目录！不要在项目目录外创建文件！

## 用户指定的经验教训
- 使用 Python venv 在 ./venv 目录
- 在程序输出中包含调试信息
- 在编辑文件前先阅读文件内容
- 所有代码、注释和文档必须使用英文
- 每个数据源模块必须独立
- 所有操作必须显示清晰的进度状态

# Scratchpad

## 当前任务：实现Apartment List空置率数据的抓取和导入

### 任务分析
这是一个数据抓取和导入任务，需要完成以下主要部分：
1. 创建Supabase表结构（已完成）
2. 实现数据抓取模块（已完成）
3. 实现数据处理模块（已完成）
4. 实现数据导入模块（已完成）
5. 更新调度器（已完成）

### 进度
[X] 创建Supabase表结构
[X] 创建数据抓取模块
  [X] 创建vacancy_index_scraper.py
  [X] 创建scrape_vacancy_index.py测试脚本
[X] 创建数据处理模块
  [X] 创建vacancy_index_processor.py
  [X] 创建process_vacancy_index.py测试脚本
[X] 创建数据导入模块
  [X] 创建import_vacancy_index.py
  [X] 添加数据验证和转换逻辑
[X] 更新调度器
  [X] 在scheduler.py中添加空置率数据的抓取、处理和导入任务

### 完成情况
1. 创建了所有必要的文件：
   - vacancy_index_scraper.py：实现数据抓取
   - scrape_vacancy_index.py：抓取脚本
   - vacancy_index_processor.py：数据处理
   - process_vacancy_index.py：处理脚本
   - import_vacancy_index.py：数据导入
   - supabase_client.py：数据库操作
2. 更新了scheduler.py以包含新的任务
3. 实现了完整的数据验证和错误处理
4. 保持了与现有代码结构的一致性

### 下一步
1. 本地测试整个流程：
   ```powershell
   # 设置PYTHONPATH
   $env:PYTHONPATH="E:\Cursor_AI\REAL_ANALYTICS_DB"

   # 测试数据抓取
   python -m src.scripts.scrape_vacancy_index

   # 测试数据处理
   python -m src.scripts.process_vacancy_index

   # 测试数据导入
   python -m src.scripts.import_vacancy_index

   # 测试调度器
   python -m src.scripts.scheduler
   ```
2. 部署到生产环境
3. 监控第一次运行的结果 